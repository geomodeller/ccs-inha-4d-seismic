{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMG_CCS_4D_Seiscmic\n",
    "\n",
    "Author: Hyunmin Kim | Last Update Date: 2024-05-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os, shutil\n",
    "from tqdm import tqdm \n",
    "from multiprocessing import Pool\n",
    "import pyvista as pv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import argparse\n",
    "from progress.bar import IncrementalBar\n",
    "\n",
    "from gan.generator import UnetGenerator\n",
    "from gan.encoder import Encoder\n",
    "from gan.discriminator import Discriminator, PatchDiscriminator, ConditionalDiscriminator\n",
    "from gan.criterion import GeneratorLoss, DiscriminatorLoss, bce_loss\n",
    "from gan.utils import Logger, initialize_weights\n",
    "from lib.slack import print_n_send\n",
    "from lib.seismogram import add_noise_to_seismic\n",
    "from lib.Sim_CMG_CCS import Sim_CCS\n",
    "from lib.subfunction import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Import the ensemble data & data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.load('ensemble_4000.npz')\n",
    "Porosity = Data['Porosity']\n",
    "Facies = Data['Facies']\n",
    "seismic = np.load('ensemble_4000_seismic_total_v3.npy')\n",
    "ref_num = 17\n",
    "\n",
    "ground_truth = np.ones((1, 2, 16, 32, 32))\n",
    "ground_truth[0, 0] = Porosity[ref_num]\n",
    "ground_truth[0, 1] = Facies[ref_num]\n",
    "ground_truth_seismic = seismic[ref_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.load('ensemble_4000.npz')\n",
    "Porosity = Data['Porosity']\n",
    "Facies = Data['Facies']\n",
    "seismic = np.load('ensemble_4000_seismic_total_v3.npy')\n",
    "ref_num = 17\n",
    "\n",
    "ground_truth = np.ones((1, 2, 16, 32, 32))\n",
    "ground_truth[0, 0] = Porosity[ref_num]\n",
    "ground_truth[0, 1] = Facies[ref_num]\n",
    "\n",
    "cdf_model = np.ones((100, 2, 16, 32, 32))\n",
    "for i in range(100):\n",
    "    cdf_model[i, 0] = Porosity[40*i]\n",
    "    cdf_model[i, 1] = Facies[40*i]\n",
    "\n",
    "ensemble = - np.ones((3999, 2, 16, 32, 32))\n",
    "ensemble[:, 0] = np.delete(Porosity, ref_num, axis=0)\n",
    "ensemble[:, 1] = np.delete(Facies, ref_num, axis=0)\n",
    "\n",
    "ground_truth_seismic = seismic[ref_num]\n",
    "seismic = np.delete(seismic, ref_num, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()\n",
    "seismic[:, 0] = scaler1.fit_transform(seismic[:, 0].reshape(-1, 1)).reshape((3999, 96, 32, 32)) * 2 - 1\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "seismic[:, 1] = scaler2.fit_transform(seismic[:, 1].reshape(-1, 1)).reshape((3999, 96, 32, 32)) * 2 - 1\n",
    "\n",
    "scaler3 = MinMaxScaler()\n",
    "seismic[:, 2] = scaler3.fit_transform(seismic[:, 2].reshape(-1, 1)).reshape((3999, 96, 32, 32)) * 2 - 1\n",
    "\n",
    "\n",
    "scaler4 = MinMaxScaler()\n",
    "ensemble[:, 0] = scaler4.fit_transform(ensemble[:, 0].reshape(-1, 1)).reshape((3999, 16, 32, 32)) * 2 - 1\n",
    "\n",
    "ensemble_resized = resize(ensemble, (3999, 2, 96, 32, 32), anti_aliasing=False)\n",
    "ensemble_resized[:, 1] = np.where(ensemble_resized[:, 1] > 0.5, 1, 0)\n",
    "ensemble_resized[:, 1] = ensemble_resized[:, 1] * 2 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: define BiCycleGAN model & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "lambda_VAE = 100; lambda_latent = 0.5; lambda_KL = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "print('Defining models!')\n",
    "generator = UnetGenerator().to(device)\n",
    "discriminator_cVAE = PatchDiscriminator().to(device)\n",
    "discriminator_cLR = PatchDiscriminator().to(device)\n",
    "encoder = Encoder(z_dim=8).to(device)\n",
    "# optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "d_cVAE_optimizer = torch.optim.Adam(discriminator_cVAE.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "d_cLR_optimizer = torch.optim.Adam(discriminator_cLR.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "e_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "# loss functions\n",
    "g_criterion = GeneratorLoss(alpha=lambda_VAE)\n",
    "d_criterion = DiscriminatorLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_state_dict(torch.load('runs\\\\generator_1184_bicycle_0719_v2.pt'))\n",
    "# encoder.load_state_dict(torch.load('runs\\\\encoder_401_bicycle_0710.pt'))\n",
    "# discriminator_cVAE.load_state_dict(torch.load('runs\\\\discriminator_cVAE_401_bicycle_0710.pt'))\n",
    "# discriminator_cLR.load_state_dict(torch.load('runs\\\\discriminator_cLR_401_bicycle_0710.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = torch.tensor(seismic.astype(np.float32))\n",
    "targets = torch.tensor(ensemble_resized.astype(np.float32))\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start of training process!')\n",
    "logger = Logger(filename='training_log')\n",
    "for epoch in range(epochs):\n",
    "    ge_loss=0.\n",
    "    de_loss=0.\n",
    "    EGe_loss=0.\n",
    "    Ge_alone_loss=0.\n",
    "    z_dim = 8\n",
    "    \n",
    "    start = time.time()\n",
    "    bar = IncrementalBar(f'[Epoch {epoch+1}/{epochs}]', max=len(data_loader))\n",
    "    for x, real in data_loader:\n",
    "        x = x.to(device)\n",
    "        real = real.to(device)\n",
    "        \n",
    "        batch_n = x.shape[0]\n",
    "        x1 = x.reshape(batch_n, 3, 96, 32, 32)\n",
    "        \n",
    "        mu, log_variance = encoder(real)\n",
    "        std = torch.exp(log_variance / 2)\n",
    "        random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "        encoded_z = (random_z * std) + mu\n",
    "        \n",
    "        # Discriminator`s loss\n",
    "        fake_cVAE = generator(x1, encoded_z).detach()\n",
    "        fake_pred_cVAE_1, fake_pred_cVAE_2 = discriminator_cVAE(fake_cVAE)\n",
    "        fake_pred_cVAE = torch.cat([fake_pred_cVAE_1.reshape(batch_n, -1), fake_pred_cVAE_2.reshape(batch_n, -1)], dim=-1)\n",
    "        real_pred_cVAE_1, real_pred_cVAE_2 = discriminator_cVAE(real)\n",
    "        real_pred_cVAE = torch.cat([real_pred_cVAE_1.reshape(batch_n, -1), real_pred_cVAE_2.reshape(batch_n, -1)], dim=-1)\n",
    "        d_loss_cVAE_1 = mse_loss(fake_pred_cVAE, target_index=0)\n",
    "        d_loss_cVAE_2 = mse_loss(real_pred_cVAE, target_index=1)\n",
    " \n",
    "        random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor))\n",
    "        fake_cLR = generator(x1, random_z).detach()\n",
    "        fake_pred_cLR_1, fake_pred_cLR_2 = discriminator_cLR(fake_cLR)\n",
    "        fake_pred_cLR = torch.cat([fake_pred_cLR_1.reshape(batch_n, -1), fake_pred_cLR_2.reshape(batch_n, -1)], dim=-1)\n",
    "        real_pred_cLR_1, real_pred_cLR_2 = discriminator_cLR(real)\n",
    "        real_pred_cLR = torch.cat([real_pred_cLR_1.reshape(batch_n, -1), real_pred_cLR_2.reshape(batch_n, -1)], dim=-1)\n",
    "        d_loss_cLR_1 = mse_loss(fake_pred_cLR, target_index=0)\n",
    "        d_loss_cLR_2 = mse_loss(real_pred_cLR, target_index=1)\n",
    "        d_loss = (d_loss_cVAE_1 + d_loss_cVAE_2 + d_loss_cLR_1 + d_loss_cLR_2)/4\n",
    "        \n",
    "        # Discriminator`s params update\n",
    "        d_cVAE_optimizer.zero_grad()\n",
    "        d_cLR_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_cVAE_optimizer.step()\n",
    "        d_cLR_optimizer.step()\n",
    "        \n",
    "        for _ in range(2):\n",
    "            # Generator`s loss\n",
    "            mu, log_variance = encoder(real)\n",
    "            std = torch.exp(log_variance / 2)\n",
    "            random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor))\n",
    "            encoded_z = (random_z * std) + mu\n",
    "            \n",
    "            fake_cVAE = generator(x1, encoded_z)\n",
    "            fake_pred_cVAE_1, fake_pred_cVAE_2 = discriminator_cVAE(fake_cVAE)\n",
    "            fake_pred_cVAE = torch.cat([fake_pred_cVAE_1.reshape(batch_n, -1), fake_pred_cVAE_2.reshape(batch_n, -1)], dim=-1)\n",
    "            g_loss_cVAE = mse_loss(fake_pred_cVAE, target_index=1)\n",
    "            \n",
    "            random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "            fake_cLR = generator(x1, random_z).detach()\n",
    "            fake_pred_cLR_1, fake_pred_cLR_2 = discriminator_cLR(fake_cLR)\n",
    "            fake_pred_cLR = torch.cat([fake_pred_cLR_1.reshape(batch_n, -1), fake_pred_cLR_2.reshape(batch_n, -1)], dim=-1)\n",
    "            g_loss_cLR = mse_loss(fake_pred_cLR, target_index=1)\n",
    "            g_loss = g_loss_cVAE + g_loss_cLR\n",
    "            \n",
    "            KL_div = lambda_KL * torch.sum(0.5 * (mu ** 2 + torch.exp(log_variance) - log_variance - 1))\n",
    "            img_recon_loss = lambda_VAE * torch.mean(torch.abs(fake_cVAE - real))\n",
    "            EG_loss = g_loss + KL_div + img_recon_loss\n",
    "            \n",
    "            d_cVAE_optimizer.zero_grad()\n",
    "            d_cLR_optimizer.zero_grad()\n",
    "            g_optimizer.zero_grad()\n",
    "            e_optimizer.zero_grad()\n",
    "            EG_loss.backward()\n",
    "            e_optimizer.step()\n",
    "            g_optimizer.step()\n",
    "        \n",
    "            # Generator's params only update\n",
    "            mu_, log_variance_ = encoder(fake_cLR)\n",
    "            z_recon_loss = torch.mean(torch.abs(mu_ - random_z))\n",
    "            G_alone_loss = lambda_latent * z_recon_loss\n",
    "            \n",
    "            d_cVAE_optimizer.zero_grad()\n",
    "            d_cLR_optimizer.zero_grad()\n",
    "            g_optimizer.zero_grad()\n",
    "            e_optimizer.zero_grad()\n",
    "            G_alone_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "        # add batch losses\n",
    "        ge_loss += g_loss.item()\n",
    "        de_loss += d_loss.item()\n",
    "        EGe_loss += EG_loss.item()\n",
    "        Ge_alone_loss += G_alone_loss.item()\n",
    "        bar.next()\n",
    "        \n",
    "    bar.finish()  \n",
    "    # obttain per epoch losses\n",
    "    g_loss = ge_loss/len(data_loader)\n",
    "    d_loss = de_loss/len(data_loader)\n",
    "    EG_loss = EGe_loss/len(data_loader)\n",
    "    G_alone_loss = Ge_alone_loss/len(data_loader)\n",
    "    # count timeframe\n",
    "    end = time.time()\n",
    "    tm = (end - start)\n",
    "    logger.add_scalar('generator_loss', g_loss, epoch+1)\n",
    "    logger.add_scalar('discriminator_loss', d_loss, epoch+1)\n",
    "    logger.add_scalar('generator_alone_loss', G_alone_loss, epoch+1)\n",
    "    logger.add_scalar('EG_loss', EG_loss, epoch+1)\n",
    "    if epoch % 200 == 0:\n",
    "        logger.save_weights(generator.state_dict(), f'generator_{epoch}_bicycle_0722')\n",
    "        logger.save_weights(discriminator_cVAE.state_dict(), f'discriminator_cVAE_{epoch}_bicycle_0722')\n",
    "        logger.save_weights(discriminator_cLR.state_dict(), f'discriminator_cLR_{epoch}_bicycle_0722')\n",
    "        logger.save_weights(encoder.state_dict(), f'encoder_{epoch}_bicycle_0722')\n",
    "    if epoch % 20 == 0:\n",
    "        print_n_send(\"Bicycle GAN: [Epoch %d/%d] [G loss: %.3f] [D loss: %.3f] [EG loss: %.3f] [G alone loss: %.3f] ETA: %.3fs\" % (epoch+1, epochs, g_loss, d_loss, EG_loss, G_alone_loss, tm))\n",
    "    else:\n",
    "        print(\"Bicycle GAN: [Epoch %d/%d] [G loss: %.3f] [D loss: %.3f] [EG loss: %.3f] [G alone loss: %.3f] ETA: %.3fs\" % (epoch+1, epochs, g_loss, d_loss, EG_loss, G_alone_loss, tm))\n",
    "logger.save_weights(generator.state_dict(), f'generator_{epoch+1}_bicycle_0722')\n",
    "logger.save_weights(discriminator_cVAE.state_dict(), f'discriminator_cVAE_{epoch+1}_bicycle_0722')\n",
    "logger.save_weights(discriminator_cLR.state_dict(), f'discriminator_cLR_{epoch+1}_bicycle_0722')\n",
    "logger.save_weights(encoder.state_dict(), f'encoder_{epoch+1}_bicycle_0722')\n",
    "logger.close()\n",
    "\n",
    "print('End of training process!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start of training process!')\n",
    "logger = Logger(filename='training_log')\n",
    "for epoch in range(epochs):\n",
    "    ge_loss=0.\n",
    "    de_loss=0.\n",
    "    EGe_loss=0.\n",
    "    Ge_alone_loss=0.\n",
    "    z_dim = 8\n",
    "    \n",
    "    start = time.time()\n",
    "    bar = IncrementalBar(f'[Epoch {epoch+1}/{epochs}]', max=len(data_loader))\n",
    "    for x, real in data_loader:\n",
    "        x = x.to(device)\n",
    "        real = real.to(device)\n",
    "        \n",
    "        batch_n = x.shape[0]\n",
    "        x1 = x.reshape(batch_n, 3, 96, 32, 32)\n",
    "        \n",
    "        mu, log_variance = encoder(real)\n",
    "        std = torch.exp(log_variance / 2)\n",
    "        random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "        encoded_z = (random_z * std) + mu\n",
    "        \n",
    "        # Discriminator`s loss\n",
    "        fake_cVAE = generator(x1, encoded_z).detach()\n",
    "        fake_pred_cVAE = discriminator_cVAE(fake_cVAE)\n",
    "        real_pred_cVAE = discriminator_cVAE(real)\n",
    "        d_loss_cVAE = d_criterion(fake_pred_cVAE, real_pred_cVAE)\n",
    " \n",
    "        random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor))\n",
    "        fake_cLR = generator(x1, random_z).detach()\n",
    "        fake_pred_cLR = discriminator_cLR(fake_cLR)\n",
    "        real_pred_cLR = discriminator_cLR(real)\n",
    "        d_loss_cLR = d_criterion(fake_pred_cLR, real_pred_cLR)\n",
    "        d_loss = d_loss_cVAE + d_loss_cLR\n",
    "        \n",
    "        # Discriminator`s params update\n",
    "        d_cVAE_optimizer.zero_grad()\n",
    "        d_cLR_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_cVAE_optimizer.step()\n",
    "        d_cLR_optimizer.step()\n",
    "        \n",
    "        # Generator`s loss\n",
    "        mu, log_variance = encoder(real)\n",
    "        std = torch.exp(log_variance / 2)\n",
    "        random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor))\n",
    "        encoded_z = (random_z * std) + mu\n",
    "        \n",
    "        fake_cVAE = generator(x1, encoded_z)\n",
    "        fake_pred_cVAE = discriminator_cVAE(fake_cVAE)\n",
    "        g_loss_cVAE = g_criterion(fake_cVAE, real, fake_pred_cVAE)\n",
    "        \n",
    "        random_z = Variable(torch.randn(batch_n, z_dim).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "        fake_cLR = generator(x1, random_z).detach()\n",
    "        fake_pred_cLR = discriminator_cLR(fake_cLR)\n",
    "        g_loss_cLR = g_criterion(fake_cLR, real, fake_pred_cLR)\n",
    "        g_loss = g_loss_cVAE + g_loss_cLR\n",
    "        \n",
    "        # Generator`s params update\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Encoder's params update\n",
    "        mu_2, log_variance_2 = encoder(real)\n",
    "        std_2 = torch.exp(log_variance_2 / 2)\n",
    "        encoded_z_2 = (random_z * std_2) + mu_2\n",
    "        fake_cVAE_2 = generator(x1, encoded_z_2)\n",
    "        fake_pred_cVAE_2 = discriminator_cVAE(fake_cVAE_2)\n",
    "        g_loss_cVAE_2 = g_criterion(fake_cVAE_2, real, fake_pred_cVAE_2)\n",
    "        \n",
    "        fake_cLR_2 = generator(x1, random_z).detach()\n",
    "        fake_pred_cLR_2 = discriminator_cLR(fake_cLR_2)\n",
    "        g_loss_cLR_2 = g_criterion(fake_cLR_2, real, fake_pred_cLR_2)\n",
    "        g_loss_2 = g_loss_cVAE_2.clone() + g_loss_cLR_2.clone()\n",
    "        \n",
    "        KL_div = lambda_KL * torch.sum(0.5 * (mu_2 ** 2 + torch.exp(log_variance_2) - log_variance_2 - 1))\n",
    "        img_recon_loss = lambda_VAE * torch.mean(torch.abs(fake_cVAE_2 - real))\n",
    "        EG_loss = g_loss_2 + KL_div + img_recon_loss\n",
    "        \n",
    "        d_cVAE_optimizer.zero_grad()\n",
    "        d_cLR_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        e_optimizer.zero_grad()\n",
    "        EG_loss.backward()\n",
    "        e_optimizer.step()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Generator's params only update\n",
    "        mu_, log_variance_ = encoder(fake_cLR)\n",
    "        z_recon_loss = torch.mean(torch.abs(mu_ - random_z))\n",
    "        G_alone_loss = lambda_latent * z_recon_loss\n",
    "        \n",
    "        d_cVAE_optimizer.zero_grad()\n",
    "        d_cLR_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        e_optimizer.zero_grad()\n",
    "        G_alone_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # add batch losses\n",
    "        ge_loss += g_loss.item()\n",
    "        de_loss += d_loss.item()\n",
    "        EGe_loss += EG_loss.item()\n",
    "        Ge_alone_loss += G_alone_loss.item()\n",
    "        bar.next()\n",
    "        \n",
    "    bar.finish()  \n",
    "    # obttain per epoch losses\n",
    "    g_loss = ge_loss/len(data_loader)\n",
    "    d_loss = de_loss/len(data_loader)\n",
    "    EG_loss = EGe_loss/len(data_loader)\n",
    "    G_alone_loss = Ge_alone_loss/len(data_loader)\n",
    "    # count timeframe\n",
    "    end = time.time()\n",
    "    tm = (end - start)\n",
    "    logger.add_scalar('generator_loss', g_loss, epoch+1)\n",
    "    logger.add_scalar('discriminator_loss', d_loss, epoch+1)\n",
    "    logger.add_scalar('generator_alone_loss', G_alone_loss, epoch+1)\n",
    "    logger.add_scalar('EG_loss', EG_loss, epoch+1)\n",
    "    if epoch % 200 == 0:\n",
    "        logger.save_weights(generator.state_dict(), f'generator_{epoch+984}_bicycle_0718_old')\n",
    "        logger.save_weights(discriminator_cVAE.state_dict(), f'discriminator_cVAE_{epoch+984}_bicycle_0718_old')\n",
    "        logger.save_weights(discriminator_cLR.state_dict(), f'discriminator_cLR_{epoch+984}_bicycle_0718_old')\n",
    "        logger.save_weights(encoder.state_dict(), f'encoder_{epoch+984}_bicycle_0718_old')\n",
    "    if epoch % 50 == 0:\n",
    "        print_n_send(\"Bicycle GAN: [Epoch %d/%d] [G loss: %.3f] [D loss: %.3f] [EG loss: %.3f] [G alone loss: %.3f] ETA: %.3fs\" % (epoch+1, epochs, g_loss, d_loss, EG_loss, G_alone_loss, tm))\n",
    "    print(\"Bicycle GAN: [Epoch %d/%d] [G loss: %.3f] [D loss: %.3f] [EG loss: %.3f] [G alone loss: %.3f] ETA: %.3fs\" % (epoch+1, epochs, g_loss, d_loss, EG_loss, G_alone_loss, tm))\n",
    "logger.save_weights(generator.state_dict(), f'generator_{epoch+984+1}_bicycle_0718_old')\n",
    "logger.save_weights(discriminator_cVAE.state_dict(), f'discriminator_cVAE_{epoch+984+1}_bicycle_0718_old')\n",
    "logger.save_weights(discriminator_cLR.state_dict(), f'discriminator_cLR_{epoch+984+1}_bicycle_0718_old')\n",
    "logger.save_weights(encoder.state_dict(), f'encoder_{epoch+984+1}_bicycle_0718_old')\n",
    "logger.close()\n",
    "\n",
    "print('End of training process!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3**: Predict the ground truth models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_real = 100\n",
    "ground_truth_pred = np.ones((N_real, 2, 16, 32, 32)) *-9999\n",
    "\n",
    "ground_truth_seismic_copy = ground_truth_seismic.copy()\n",
    "ground_truth_seismic_copy[0] = scaler1.transform(ground_truth_seismic_copy[0].reshape(-1, 1)).reshape(96, 32, 32)*2-1\n",
    "ground_truth_seismic_copy[1] = scaler2.transform(ground_truth_seismic_copy[1].reshape(-1, 1)).reshape(96, 32, 32)*2-1\n",
    "ground_truth_seismic_copy[2] = scaler3.transform(ground_truth_seismic_copy[2].reshape(-1, 1)).reshape(96, 32, 32)*2-1\n",
    "ground_truth_seismic_copy = ground_truth_seismic_copy.reshape(1, 3, 96, 32, 32)\n",
    "ground_truth_seismic_copy = torch.tensor(ground_truth_seismic_copy.astype(np.float32), device = 'cuda').reshape(1, 3, 96, 32, 32)\n",
    "ground_truth_seismic_copy.to(device)\n",
    "\n",
    "for i in range(N_real):\n",
    "    random_z = Variable(torch.randn(1, 8).type(torch.cuda.FloatTensor))\n",
    "    out = np.array(generator(ground_truth_seismic_copy, random_z).detach().cpu())\n",
    "    out = upscale_3d(out)\n",
    "    print(f'{i}# prediction model is generated')\n",
    "\n",
    "    out[:, 1] = (out[:, 1] + 1) / 2\n",
    "    out[:, 1] = np.where(out[:, 1] > 0.5, 1, 0)\n",
    "    out[:, 0] = cdf_mapping_by_facies(out[:, 0], cdf_model[:, 0], out[:, 1], cdf_model[:, 1])\n",
    "    \n",
    "    ground_truth_pred[i] = out[0]\n",
    "\n",
    "ground_truth_pred = np.concatenate([ground_truth_pred, por_to_perm(ground_truth_pred[:, 0], ground_truth_pred[:, 1]).reshape(N_real, 1, 16, 32, 32)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.concatenate([ground_truth, por_to_perm(ground_truth[:, 0], ground_truth[:, 1]).reshape(1, 1, 16, 32, 32)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ground_truth_pred = ground_truth_pred.mean(axis=0)\n",
    "# Plot ensemble model & prediction model\n",
    "N_real = 0\n",
    "plotter = pv.Plotter(shape = (2, 3), window_size = [1024, 1024], border=False)\n",
    "\n",
    "plotter.subplot(0, 0)\n",
    "visual_grid_1 = ground_truth[0, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.add_title('True', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "visual_grid_2 = ground_truth[0, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, show_scalar_bar=True, scalar_bar_args={'title':'Porosity [fraction]', 'title_font_size':15,\n",
    "                                                            'label_font_size':10})\n",
    "plotter.add_title('Prediction', font_size=12)\n",
    "plotter.update_scalar_bar_range([0, 0.35])\n",
    "\n",
    "plotter.subplot(0, 2)\n",
    "visual_grid_1 = ground_truth[0, 2]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "# slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=True, cmap='jet', scalar_bar_args={'title':'Permeability [mD]', 'title_font_size':15,\n",
    "                                                            'label_font_size':10})\n",
    "plotter.update_scalar_bar_range([0, 150])\n",
    "\n",
    "\n",
    "plotter.subplot(1, 0)\n",
    "visual_grid_2 = avg_ground_truth_pred[1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "# slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "\n",
    "plotter.subplot(1, 1)\n",
    "visual_grid_2 = avg_ground_truth_pred[0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "# slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.update_scalar_bar_range([0, 0.35])\n",
    "\n",
    "plotter.subplot(1, 2)\n",
    "visual_grid_2 = avg_ground_truth_pred[2]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "# slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=False, cmap='jet')\n",
    "plotter.update_scalar_bar_range([0, 150])\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter(shape = (4, 4), window_size = [1024, 1024], border=False)\n",
    "\n",
    "for i in range(16):\n",
    "    plotter.subplot(i//4, i%4)\n",
    "    visual_grid_2 = ground_truth_pred[i, 1]\n",
    "    grid = pv.ImageData()\n",
    "    grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "    grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "    grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "    grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "    plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4**: PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model = np.ones((4000, 2, 16, 32, 32))\n",
    "PCA_model[:, 0] = Porosity\n",
    "PCA_model[:, 1] = Facies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # 주성분을 몇개로 할지 결정\n",
    "printcipalComponents = pca.fit_transform(PCA_model.reshape(4000, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = [[10*i]*40 for i in range(10)]\n",
    "color = np.repeat(np.array(color).reshape(1, 10, 40), 10, axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_num = 20\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.scatter(printcipalComponents[ref_num, 0], printcipalComponents[ref_num, 1], c='None', edgecolors='r', marker='^', s=50, alpha=.7, label='Reference', zorder=2)\n",
    "plt.scatter(printcipalComponents[:, 0], printcipalComponents[:, 1], c=color, s=3, cmap='grey', alpha=.7, label='Initial ensemble', zorder=1)\n",
    "plt.xlabel('1st Printcipal Component')\n",
    "plt.ylabel('2nd Printcipal Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pca = pca.transform(ground_truth_pred[:, :2].reshape(100, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(printcipalComponents[0, 0], printcipalComponents[0, 1], c='None', edgecolors='r', marker='^', s=50, alpha=.7, label='Reference')\n",
    "plt.scatter(printcipalComponents[:, 0], printcipalComponents[:, 1], c='grey', s=3, alpha=.3, label='Initial ensemble')\n",
    "plt.scatter(prediction_pca[:, 0], prediction_pca[:, 1], c='b', s=3, alpha=.8, label='prediction')\n",
    "plt.xlabel('1st Printcipal Component')\n",
    "plt.ylabel('2nd Printcipal Component')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5**: Comparison of dynamic property (gas saturation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NX, NY, NZ = 32, 32, 16\n",
    "# instantiate the simulation object\n",
    "sim_ccs = Sim_CCS()\n",
    "\n",
    "# set the simulation parameters\n",
    "input_parameters = {'porosity': ground_truth_pred[:, 0],\n",
    "                    'permeability': ground_truth_pred[:, 2],\n",
    "                    'Facies': ground_truth_pred[:, 1]}\n",
    "\n",
    "# set the bottom hole pressure constraint\n",
    "maximum_bhp = 3500 # psi\n",
    "sim_ccs.reset_bhp_constraint(maximum_bhp)\n",
    "\n",
    "# set the injection amount\n",
    "injection_rate = 15_839_619 # scf/day\n",
    "sim_ccs.reset_injection_amout(injection_rate)\n",
    "\n",
    "# set your CMG exe path\n",
    "cmg_exe = '\"C:\\\\Program Files\\\\CMG\\\\GEM\\\\2024.20\\\\Win_x64\\\\EXE\\\\gm202420.exe\"'\n",
    "sim_ccs.reset_parameters_to_play(list(input_parameters.keys()))\n",
    "sim_ccs.reset_CMG_exe(cmg_exe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the simulation cases of the models generated by BicycleGAN\n",
    "sim_ccs.ensemble_simulation(sub_dir='ground_truth_pred\\\\pred' , input_parameters=input_parameters)\n",
    "\n",
    "# TODO: Template data 시뮬 시간을 7년으로 연장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulation with multiprocessing\n",
    "sim_ccs.run_multiple_CMG(N_thread=10, sub_dir = os.path.join(os.getcwd(), 'ground_truth_pred\\\\pred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract gas saturation from results\n",
    "sim_ccs.reset_Ptime([2025, 2026, 2027, 2028, 2029, 2030, 2031])\n",
    "\n",
    "ground_truth_pred_Sg = np.ones((100, 8, 16, 32, 32)) * -9999\n",
    "for i in range(100):\n",
    "    ground_truth_pred_Sg[i] =  sim_ccs.read_grid_results(run_dir=f'ground_truth_pred\\\\pred_{i}')['GasSaturation']\n",
    "\n",
    "ground_truth_pred_Sg_mean = ground_truth_pred_Sg.mean(axis=0)\n",
    "ground_truth_pred_Sg_std = ground_truth_pred_Sg.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ensemble model & prediction model\n",
    "plotter = pv.Plotter(shape = (2, 2), window_size = [1024, 1024], border=False)\n",
    "\n",
    "plotter.subplot(0, 0)\n",
    "visual_grid_1 = ground_truth_pred_Sg_mean[4]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=True, cmap='binary', scalar_bar_args={'title':'Gas saturation', 'title_font_size':15,\n",
    "                                                            'label_font_size':10, 'position_x': 0.1,\n",
    "                                                            'width':0.3})\n",
    "plotter.add_title('Gas saturation after CO2 injection 5 years', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "visual_grid_2 = ground_truth_pred_Sg_mean[-1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='binary')\n",
    "\n",
    "plotter.add_title('Gas saturation after CO2 injection 7 years', font_size=12)\n",
    "\n",
    "plotter.subplot(1, 0)\n",
    "visual_grid_1 = ground_truth_pred_Sg_std[4]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=True, cmap='jet', scalar_bar_args={'title':'std in gas saturation', 'title_font_size':15,\n",
    "                                                            'label_font_size':10})\n",
    "\n",
    "plotter.subplot(1, 1)\n",
    "visual_grid_1 = ground_truth_pred_Sg_std[-1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=True, cmap='jet', scalar_bar_args={'title':'std in gas saturation', 'title_font_size':15,\n",
    "                                                            'label_font_size':10})\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6**: Signal_to_noise stress analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_list = [30, 50, 80]\n",
    "ground_truth_seismic_with_noise = np.ones((len(SNR_list), 3, 96, 32, 32))\n",
    "for _, SNR in enumerate(SNR_list):\n",
    "    ground_truth_seismic_with_noise[_, 0] = add_noise_to_seismic(ground_truth_seismic[0], SNR)\n",
    "    ground_truth_seismic_with_noise[_, 1] = add_noise_to_seismic(ground_truth_seismic[1], SNR)\n",
    "    ground_truth_seismic_with_noise[_, 2] = ground_truth_seismic_with_noise[_, 1] - ground_truth_seismic_with_noise[_, 0]\n",
    "    print(f'{SNR} case is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter(shape = (3, 4), window_size = [1024, 1024], border=False)\n",
    "\n",
    "plotter.subplot(0, 0)\n",
    "visual_grid_1 = ground_truth_seismic[0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.add_title('SNR: 1.0', font_size=12)\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "visual_grid_2 = ground_truth_seismic_with_noise[2, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.add_title('SNR: 0.8', font_size=12)\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(0, 2)\n",
    "visual_grid_1 = ground_truth_seismic_with_noise[1, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.add_title('SNR: 0.5', font_size=12)\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(0, 3)\n",
    "visual_grid_1 = ground_truth_seismic_with_noise[0, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.add_title('SNR: 0.3', font_size=12)\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "\n",
    "plotter.subplot(1, 0)\n",
    "visual_grid_1 = ground_truth_seismic[1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(1, 1)\n",
    "visual_grid_2 = ground_truth_seismic_with_noise[2, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(1, 2)\n",
    "visual_grid_1 = ground_truth_seismic_with_noise[1, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(1, 3)\n",
    "visual_grid_1 = ground_truth_seismic_with_noise[0, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(2, 0)\n",
    "visual_grid_1 = ground_truth_seismic[2]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(2, 1)\n",
    "visual_grid_2 = ground_truth_seismic_with_noise[2, 2]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(2, 2)\n",
    "visual_grid_1 = ground_truth_seismic_with_noise[1, 2]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.subplot(2, 3)\n",
    "visual_grid_1 = ground_truth_seismic_with_noise[0, 2]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1/6)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(slice, show_scalar_bar=False, cmap='seismic')\n",
    "plotter.update_scalar_bar_range([-0.4, 0.4])\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_seismic_with_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_real = 100\n",
    "ground_truth_with_noise_pred = np.ones((len(SNR_list), N_real, 2, 16, 32, 32)) *-9999\n",
    "\n",
    "for _, SNR in enumerate(SNR_list):\n",
    "    ground_truth_seismic_copy = ground_truth_seismic_with_noise[_].copy()\n",
    "    ground_truth_seismic_copy[0] = scaler1.transform(ground_truth_seismic_copy[0].reshape(-1, 1)).reshape(96, 32, 32)*2-1\n",
    "    ground_truth_seismic_copy[1] = scaler2.transform(ground_truth_seismic_copy[1].reshape(-1, 1)).reshape(96, 32, 32)*2-1\n",
    "    ground_truth_seismic_copy[2] = scaler3.transform(ground_truth_seismic_copy[2].reshape(-1, 1)).reshape(96, 32, 32)*2-1\n",
    "    ground_truth_seismic_copy = ground_truth_seismic_copy.reshape(1, 3, 96, 32, 32)\n",
    "    ground_truth_seismic_copy = torch.tensor(ground_truth_seismic_copy.astype(np.float32), device = 'cuda').reshape(1, 3, 96, 32, 32)\n",
    "    ground_truth_seismic_copy.to(device)\n",
    "\n",
    "    for i in range(N_real):\n",
    "        random_z = Variable(torch.randn(1, 8).type(torch.cuda.FloatTensor))\n",
    "        out = np.array(generator(ground_truth_seismic_copy, random_z).detach().cpu())\n",
    "        out = upscale_3d(out)\n",
    "        print(f'{i}# prediction model of {SNR} SNR ratio is generated')\n",
    "\n",
    "        out[:, 1] = (out[:, 1] + 1) / 2\n",
    "        out[:, 1] = np.where(out[:, 1] > 0.5, 1, 0)\n",
    "        out[:, 0] = cdf_mapping_by_facies(out[:, 0], cdf_model[:, 0], out[:, 1], cdf_model[:, 1])\n",
    "        \n",
    "        ground_truth_with_noise_pred[_, i] = out[0]\n",
    "\n",
    "# ground_truth_pred = np.concatenate([ground_truth_pred, por_to_perm(ground_truth_pred[:, :, ], ground_truth_pred[:, 1]).reshape(N_real, 1, 16, 32, 32)], axis=1)\n",
    "avg_ground_truth_with_noise_pred = ground_truth_with_noise_pred.mean(axis=1)\n",
    "std_ground_truth_with_noise_pred = ground_truth_with_noise_pred.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average of prediction model with SNR\n",
    "plotter = pv.Plotter(shape = (2, 4), window_size = [1024, 1024], border=False)\n",
    "\n",
    "\n",
    "'''------------------------------Avg.facies plot------------------------------'''\n",
    "plotter.subplot(0, 0)\n",
    "visual_grid_1 = ground_truth_pred.mean(axis=0)[1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 1.0', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "visual_grid_2 = avg_ground_truth_with_noise_pred[0, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 0.8', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 2)\n",
    "visual_grid_1 = avg_ground_truth_with_noise_pred[1, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "# slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 0.5', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 3)\n",
    "visual_grid_1 = avg_ground_truth_with_noise_pred[2, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 0.3', font_size=12)\n",
    "\n",
    "'''------------------------------Avg.porosity plot------------------------------'''\n",
    "plotter.subplot(1, 0)\n",
    "visual_grid_1 = ground_truth_pred.mean(axis=0)[0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.update_scalar_bar_range([0, 0.35])\n",
    "\n",
    "plotter.subplot(1, 1)\n",
    "visual_grid_2 = avg_ground_truth_with_noise_pred[0, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.update_scalar_bar_range([0, 0.35])\n",
    "\n",
    "plotter.subplot(1, 2)\n",
    "visual_grid_1 = avg_ground_truth_with_noise_pred[1, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.update_scalar_bar_range([0, 0.35])\n",
    "\n",
    "plotter.subplot(1, 3)\n",
    "visual_grid_1 = avg_ground_truth_with_noise_pred[2, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, show_scalar_bar=False)\n",
    "plotter.update_scalar_bar_range([0, 0.35])\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average of prediction model with SNR\n",
    "plotter = pv.Plotter(shape = (2, 4), window_size = [1024, 1024], border=False)\n",
    "\n",
    "\n",
    "'''------------------------------Avg.facies plot------------------------------'''\n",
    "plotter.subplot(0, 0)\n",
    "visual_grid_1 = ground_truth_pred.std(axis=0)[1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 1.0', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "visual_grid_2 = std_ground_truth_with_noise_pred[0, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 0.8', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 2)\n",
    "visual_grid_1 = std_ground_truth_with_noise_pred[1, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "# slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 0.5', font_size=12)\n",
    "\n",
    "plotter.subplot(0, 3)\n",
    "visual_grid_1 = std_ground_truth_with_noise_pred[2, 1]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "plotter.add_title('SNR: 0.3', font_size=12)\n",
    "\n",
    "'''------------------------------Avg.porosity plot------------------------------'''\n",
    "plotter.subplot(1, 0)\n",
    "visual_grid_1 = ground_truth_pred.std(axis=0)[0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "slice = grid.slice_orthogonal(x=9, y=9, z=2)\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "\n",
    "plotter.subplot(1, 1)\n",
    "visual_grid_2 = std_ground_truth_with_noise_pred[0, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_2[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_2[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "\n",
    "plotter.subplot(1, 2)\n",
    "visual_grid_1 = std_ground_truth_with_noise_pred[1, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "\n",
    "plotter.subplot(1, 3)\n",
    "visual_grid_1 = std_ground_truth_with_noise_pred[2, 0]\n",
    "grid = pv.ImageData()\n",
    "grid.dimensions = np.array(visual_grid_1[::-1].T.shape) + 1\n",
    "grid.origin = (1, 1, 1)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 1, 1)  # These are the cell sizes along each axis\n",
    "grid.cell_data[\"values\"] = visual_grid_1[::-1].T.flatten(order=\"F\")  # Flatten the array\n",
    "plotter.add_mesh(grid, cmap='jet', show_scalar_bar=False)\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, SNR in enumerate(SNR_list):\n",
    "    plt.hist(std_ground_truth_with_noise_pred[_, 0].flatten(), bins=30, density=True, alpha=.6, label=f'SNR: {SNR}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **APPENDIX**: Loss curve check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(os.getcwd(), 'runs', 'July_20_2024_12_56AM_training_log.json'))\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "for key in ['discriminator_loss', 'generator_alone_loss']:\n",
    "    epochs = [int(epoch) for epoch in data[key].keys()]\n",
    "    values = [float(value) for value in data[key].values()]\n",
    "\n",
    "    plt.plot(epochs, values, label=f'{key}')\n",
    "\n",
    "plt.xlabel('Epoch', fontdict={'fontsize':12})\n",
    "plt.ylabel('Loss', fontdict={'fontsize':12})\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "for key in ['generator_loss', 'EG_loss']:\n",
    "    epochs = [int(epoch) for epoch in data[key].keys()]\n",
    "    values = [float(value) for value in data[key].values()]\n",
    "\n",
    "    plt.plot(epochs, values, label=f'{key}')\n",
    "\n",
    "plt.xlabel('Epoch', fontdict={'fontsize':12})\n",
    "plt.ylabel('Loss', fontdict={'fontsize':12})\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
